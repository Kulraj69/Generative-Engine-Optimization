# Datasets

This directory contains datasets, benchmarks, and evaluation resources for Generative Engine Optimization (GEO).

## üìä Benchmark Datasets

### GEO-Bench Dataset
- **Name**: GEO-Bench
- **Description**: Comprehensive benchmark dataset for Generative Engine Optimization research
- **Availability**: Public dataset on Hugging Face
- **Structure**: 
  - Training/validation/test splits
  - Example queries
  - Evaluation metrics
- **Access**: [Hugging Face GEO-Bench](https://huggingface.co/datasets/geo-bench)
- **Use Case**: Research and development of GEO optimization methods

### Dataset Features
- **Query Types**: Various types of user queries for testing optimization strategies
- **Ground Truth**: Annotated data for evaluation and comparison
- **Metrics**: Standardized evaluation criteria for GEO performance
- **Documentation**: Comprehensive documentation for dataset usage

## üî¨ Evaluation Datasets

### Research Datasets
- **GEO Research Dataset** - Used in the main GEO research paper
  - **Source**: Aggarwal et al. (2024)
  - **Purpose**: Evaluating optimization strategies for generative engines
  - **Findings**: Demonstrates 40%+ improvement in citations with proper optimization
  - **Access**: Available through the GEO research project

### Synthetic Data
- **Prompt Generation Data** - Synthetic prompts for brand analysis
  - **Usage**: Generated by platforms like Profound for brand presence analysis
  - **Application**: Testing how different prompts affect brand visibility
  - **Method**: AI-generated prompts to simulate user queries

## üìà Performance Metrics

### Citation Metrics
- **Citation Rate** - Percentage of queries where content is cited
- **Citation Quality** - Relevance and accuracy of citations
- **Brand Visibility** - Frequency of brand mentions in AI responses

### Content Metrics
- **Content Relevance** - How well content matches user intent
- **Authority Score** - Trustworthiness and credibility measures
- **Engagement Metrics** - User interaction with cited content

## üõ†Ô∏è Dataset Tools

### Access and Processing
- **Hugging Face Integration** - Easy access through Hugging Face datasets
- **Python APIs** - Standard dataset loading and processing
- **Evaluation Scripts** - Pre-built evaluation and benchmarking tools

### Analysis Tools
- **GEO Research Code** - Official implementation for dataset analysis
- **Benchmarking Framework** - Black-box optimization framework
- **Baseline Strategies** - Standard approaches for comparison

## üìù Adding New Datasets

When adding a new dataset:

1. Create a new markdown file with the dataset name (kebab-case)
2. Include the following information:
   - Dataset name and description
   - Access method and licensing
   - Data structure and format
   - Use cases and applications
   - Citation and attribution
3. Update this README with a reference to the new file
4. Add the dataset to the main repository README.md

## üîó External Resources

- [Hugging Face Datasets](https://huggingface.co/datasets) - Dataset repository
- [Papers With Code](https://paperswithcode.com/) - Datasets with papers
- [Kaggle](https://www.kaggle.com/) - Data science datasets
- [Google Dataset Search](https://datasetsearch.research.google.com/) - Dataset discovery
- [GEO Project Website](https://generative-engines.com/GEO/) - Official GEO resources
